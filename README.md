# Apresentação Pessoal

<img src="https://avatars.githubusercontent.com/u/126106533?s=400&u=570115129a481f0705ced955273b889c44dc3722&v=4" width="75px" style="margin: 0 15px 0 0; border-radius: 99%" align="left">

<br> 🗓️ Rodrigo da Silva Pereira

<p>
  👋 Olá, eu sou o Rodrigo! Um estudante de Desenvolvimento de Software Multiplataforma.

  - 🌱 Tenho 19 anos e sou residente na cidade de Cajati, no interior de São Paulo.
  
  - 🎓 Atualmente estou cursando o 5° semestre de Desenvolvimento de Software Multiplataforma na Fatec Registro.

  - 🔭 Iniciei minha jornada profissional em março de 2024, através do programa de bolsas Compass UOL, trabalhando na área de AWS Clouding.

  - 💬 Possuo conhecimentos intermediários em inglês e experiência prática em SQL, NoSQL, Java, Python, engenharia e análise de dados. Atualmente, sigo em busca de progredir em minha carreira profissional e aprofundar meus conhecimentos em Clouding e Data.

  - ⚡ Além disso, sou apaixonado por leitura, absorvendo diversos tipos de conteúdos, e também gosto de passar meu tempo livre jogando.

  - 🌟 Sou uma pessoa dedicada, sempre em busca de aprendizado contínuo. Acredito que somos capazes de aprender tudo aquilo que nos interessa.
</p>

# Progresso e Conhecimentos

Durante meu estágio na Compass UOL, tive a oportunidade de mergulhar em diversos aspectos da área de TI e enfrentar desafios empolgantes. Abaixo, compartilho alguns dos principais aprendizados e experiências que adquiri nessa etapa profissional.

## Sprint 1

### Aprendizados na Primeira Sprint

Durante a primeira sprint, tive a oportunidade de participar de um curso introdutório sobre Linux - Ubuntu e Git - Github. Abaixo, compartilho um resumo dos principais tópicos que aprendi:

#### Linux - Ubuntu
- 🖥️ Instalação e configuração básica do sistema operacional Linux Ubuntu.
- 🖥️ Comandos básicos do terminal, como navegar entre diretórios, criar e excluir arquivos e diretórios, entre outros.
- 📦 Gerenciamento de pacotes usando o apt-get para instalar, atualizar e remover programas.
- 🔒 Noções básicas de permissões de arquivo e segurança.

#### Git - Github
- 🌀 Conceitos fundamentais de controle de versão e a importância do Git.
- 🌀 Inicialização de um repositório Git local e o processo de commit.
- 🌐 Uso do GitHub para hospedar repositórios remotos e colaborar com outros desenvolvedores.
- 🌐 Criação e gerenciamento de branches para trabalhar em paralelo em diferentes recursos.

Esses aprendizados foram essenciais para minha jornada como desenvolvedor e me prepararam para os desafios que enfrentarei em minha jornada de trabalho.

[Sprint 1](Sprint%201/README.md)
   
## Sprint 2

### Aprendizados na Segunda Sprint

Durante a segunda sprint, aproveitei a oportunidade para me aprofundar em um curso completo de SQL. Abaixo, compartilho um resumo dos principais tópicos que aprendi:

#### SQL - Do básico ao Avançado
- 📚 Aprendizado dos conceitos fundamentais do SQL, incluindo consultas básicas, inserção, atualização e exclusão de dados.
- 📊 Compreensão aprofundada das cláusulas SELECT, WHERE, JOIN e GROUP BY para realizar consultas complexas em bancos de dados.
- 💼 Domínio das técnicas de modelagem de dados e normalização para projetar e otimizar a estrutura de bancos de dados.
- 🔍 Exploração de funções avançadas do SQL, como subconsultas, funções de agregação, procedimentos armazenados e gatilhos.
- 📈 Implementação de estratégias de otimização de consultas para melhorar o desempenho e eficiência das operações no banco de dados.
- 📝 Realização de projetos práticos para aplicar os conhecimentos adquiridos em situações do mundo real.

Esses conhecimentos adquiridos foram fundamentais para minha trajetória como desenvolvedor e me capacitaram para enfrentar os desafios que surgirão ao longo da minha carreira!

[Sprint 2](Sprint%202/README.md)

## Sprint 3

### Aprendizado na Terceira Sprint

Python: Explorando e Aprendendo: Durante esta etapa do meu aprendizado, mergulhei fundo no mundo da programação em Python. Abaixo, destaco algumas das atividades que realizei e o que aprendi:

#### Manipulação de Dados com Pandas:
- 📊 Aprendi a carregar, limpar e manipular dados de diversas fontes usando a biblioteca Pandas.
- 📈 Explorei técnicas avançadas de filtragem, seleção e agrupamento de dados para análise.
- 📉 Utilizei Pandas para realizar cálculos estatísticos e criar visualizações informativas a partir dos dados.
#### Visualização de Dados com Matplotlib
- 📊 Criei gráficos de linhas, barras, dispersão e pizza para representar visualmente os dados.
- 📈 Personalizei os gráficos com cores, rótulos, legendas e outros elementos para torná-los mais compreensíveis e atraentes.
- 📉 Explorei diferentes estilos e tipos de gráficos para comunicar efetivamente insights a partir dos dados.
#### Projetos de Análise de Dados
- 📊 Trabalhei em projetos práticos para analisar conjuntos de dados do mundo real e extrair insights valiosos.
- 📈 Utilizei Python para responder a perguntas específicas e resolver problemas analíticos usando os dados disponíveis.
- 📉 Apliquei técnicas de visualização e análise de dados para comunicar resultados de forma clara e eficaz.

Essas experiências me proporcionaram uma compreensão mais profunda do Python como uma ferramenta poderosa para análise de dados e visualização. Estou entusiasmado para continuar explorando e expandindo meu conhecimento neste domínio emocionante da programação!

[Sprint 3](Sprint%203/README.md)

## Sprint 4

### Aprendizado na Quarta Sprint

Docker: Explorando e Aprendendo: Durante este curso, mergulhei fundo no universo do Docker, uma ferramenta essencial para o desenvolvimento e implantação de aplicativos em contêineres. Abaixo, destaco algumas das atividades que realizei e o que aprendi:

### Construção e Gerenciamento de Contêineres com Docker:
- 🐳 Aprendi a criar, gerenciar e distribuir contêineres usando Docker, uma ferramenta fundamental para a implantação de aplicativos modernos.
- 📦 Explorei técnicas avançadas de configuração e otimização de contêineres para diferentes ambientes de implantação.
- 🚀 Utilizei Docker para criar imagens eficientes e escaláveis para aplicativos em contêineres.
### Orquestração de Contêineres com Docker Compose e Kubernetes:
- 🐳 Configurei e gerenciei aplicativos multi-contêineres usando Docker Compose, simplificando o desenvolvimento e implantação de aplicações complexas.
- ⚙️ Explorei o Kubernetes como uma plataforma de orquestração de contêineres para dimensionamento automático, tolerância a falhas e implantação simplificada de aplicativos em contêineres.
- 🌐 Aprendi a usar recursos avançados do Kubernetes para implantar, atualizar e monitorar aplicativos em escala.
### Implantação de Aplicações em Ambientes de Produção:
- 🐳 Trabalhei em projetos práticos para implantar e gerenciar aplicativos Docker em ambientes de produção.
- ⚙️ Utilizei técnicas de monitoramento e escalabilidade para garantir o desempenho e a disponibilidade dos aplicativos em contêineres.
- 🌐 Apliquei práticas recomendadas de segurança e manutenção para garantir a integridade e a confiabilidade das implantações em contêineres.

Essas experiências me proporcionaram uma compreensão mais profunda do Docker como uma ferramenta poderosa para o desenvolvimento, implantação e gerenciamento de aplicativos em contêineres.

[Sprint 4](Sprint%204/README.md)

## Sprint 5

### Aprendizado na Quinta Sprint

Durante a quinta sprint, concentrei-me em explorar e aprofundar meus conhecimentos em AWS S3 com Boto3. Abaixo, destaco algumas das atividades que realizei e o que aprendi:

### Construção e Gerenciamento de Serviços S3:
- 🛠️ Aprendi a interagir com o Amazon Simple Storage Service (S3) usando a biblioteca Boto3, uma ferramenta essencial para manipulação de objetos no armazenamento em nuvem da AWS.
- 📂 Explorei técnicas para listar, carregar, baixar e excluir objetos em buckets do S3 utilizando operações programáticas.
- 🔄 Utilizei as capacidades de controle de acesso do S3 para gerenciar permissões de acesso a buckets e objetos, garantindo a segurança dos dados armazenados.
### Automatização de Tarefas com AWS S3 e Boto3:
- ⚙️ Desenvolvi scripts Python para automatizar tarefas rotineiras de gerenciamento de dados no S3, como sincronização de arquivos, cópias de segurança e monitoramento de eventos.
- 🤖 Explorei a integração do AWS S3 com outros serviços da AWS, como Lambda, SNS e CloudWatch, para construir pipelines de dados automatizados e escaláveis.

Essas experiências me proporcionaram uma compreensão mais profunda do AWS S3 e do Boto3 como ferramentas poderosas para armazenamento e gerenciamento de dados na nuvem da AWS. Estou entusiasmado para continuar explorando e expandindo meu conhecimento neste domínio emocionante da computação em nuvem e DevOps!

[Sprint 5](Sprint%205/README.md)

## Sprint 6

### Aprendizado na Sexta Sprint

Durante a sexta sprint, concentrei-me em consolidar e expandir meus conhecimentos em AWS S3 com Boto3, além de explorar novos desafios e conceitos. Abaixo, destaco algumas das atividades que realizei e o que aprendi:

### Manipulação Avançada de Dados no AWS S3:
- 🛠️ Aprofundei minha compreensão sobre a manipulação de dados no Amazon Simple Storage Service (S3) usando a biblioteca Boto3, explorando técnicas avançadas para operações de leitura, escrita e manipulação de objetos em buckets do S3.
- 📂 Trabalhei com operações avançadas de gerenciamento de objetos, como controle de versões, criptografia de dados, e utilização de metadados para categorização e organização eficiente de dados armazenados.
- 🔄 Aprendi sobre a implementação de pipelines de dados mais complexos e robustos, incluindo a integração do AWS S3 com serviços de processamento de dados, como AWS Glue e Amazon EMR.

Essas experiências me proporcionaram uma base sólida e abrangente em AWS S3 e Boto3, capacitando-me a enfrentar desafios cada vez mais complexos na área de armazenamento e gerenciamento de dados na nuvem!

[Sprint 6](Sprint%206/README.md)

## Sprint 7

### Aprendizado na Sétima Sprint

Durante a sétima sprint, concentrei-me na criação de um desafio envolvendo AWS Lambda para processamento de dados no Amazon S3. Abaixo, destaco as principais atividades realizadas e o aprendizado adquirido:

### Integração de AWS Lambda com Amazon S3
- 🛠️ **Desenvolvimento**: Criei um script Python para integração entre AWS Lambda e Amazon S3.
- 🔍 **Funcionalidades**: A função Lambda lê dados de um arquivo CSV no S3, processa registros de filmes baseado em critérios específicos e salva os dados filtrados em JSON de volta no S3.
- 🖥️ **Tecnologias Utilizadas**: Boto3 para interação com o S3, AWS Lambda para transformação automação e integração.

Essas experiências me proporcionaram uma base sólida e abrangente em S3 e Lambda, capacitando-me a enfrentar desafios cada vez mais complexos na área de armazenamento e gerenciamento de dados na nuvem. Estou animado para continuar expandindo meu conhecimento e explorando novas possibilidades neste domínio fascinante da computação em nuvem e DevOps!

[Sprint 7](Sprint%207/README.md)

## Sprint 8

### Aprendizado na Oitava Sprint

Durante a oitava sprint, concentrei-me na criação de um desafio envolvendo AWS Glue e Spark para processamento de dados no Amazon S3. Abaixo, destaco as principais atividades realizadas e o aprendizado adquirido:

### Integração de AWS Glue com Amazon S3
- 🛠️ **Desenvolvimento**: Criei scripts Python para integração entre AWS Glue e Amazon S3.
- 🔍 **Funcionalidades**: Os scripts leem dados de arquivos CSV e JSON no S3, processam os registros com base em critérios específicos (como gênero 'Sci-Fi' e 'Fantasy' para CSV) e salvam os dados transformados em formato Parquet de volta no S3.
- 🖥️ **Tecnologias Utilizadas**: PySpark para processamento de dados, AWS Glue para automação e integração, e Amazon S3 para armazenamento.

Essas experiências me proporcionaram uma base sólida e abrangente em AWS Glue e Spark, capacitando-me a enfrentar desafios cada vez mais complexos na área de processamento e gerenciamento de dados na nuvem.

[Sprint 8](Sprint%208/README.md)

## Sprint 9

### Aprendizado na Nona Sprint

Durante a nota sprint, concentrei-me no refinamento dos dados que seriam utilizados na visualização:

### Integração de AWS Glue com Amazon S3
- 🛠️ **Refinamento**: Refinamos dados da camada "trusted" para garantir que os dados utilizados na visualização sejam precisos e confiáveis.
- 🔍 **Leitura de Parquet**: Lemos os arquivos Parquet gerados pelo AWS Glue e processamos os dados para garantir que eles estejam na forma correta para a visualização.
- 🖥️ **Tecnologias Utilizadas**: PySpark para processamento de dados, AWS Glue para automação e integração, e Amazon S3 para armazenamento.

Este processo envolveu a leitura dos arquivos Parquet, a junção dos dados das fontes TMDB e API, a exploração da coluna "genero_filme" para desaninhar os gêneros de cada filme, seleção e renomeação de colunas para garantir consistência nos dados. O resultado foi armazenado de volta no S3, estruturado e pronto para ser utilizado em análises subsequentes utilizando serviços como Athena e QuickSight.

[Sprint 9](Sprint%209/README.md)

## Sprint 10

### Aprendizado na Décima

Durante a décima sprint, concentrei-me na criação e otimização de dashboards no AWS QuickSight, utilizando dados refinados e armazenados no Amazon S3. Abaixo, destaco as principais atividades realizadas e o aprendizado adquirido:

### Criação de Dashboards com AWS QuickSight
- 📊 **Desenvolvimento**: Utilizei AWS QuickSight para criar dashboards detalhados e visualmente atraentes, baseados nos dados tratados nas sprints anteriores. O foco principal foi na análise de subgêneros do gênero "Speculative".
- 📈 **Funcionalidades**: Realizei análises aprofundadas sobre os subgêneros dos filmes, identificando a frequência e outras métricas importantes para fornecer insights valiosos.
- 🖥️ **Tecnologias Utilizadas**: Amazon S3 para armazenamento escalável de dados, Amazon Athena para consultas SQL na nuvem e AWS QuickSight para visualizações interativas e detalhadas.

Essas experiências me proporcionaram um entendimento mais profundo de visualização de dados, análise de dados em nuvem e criação de dashboards interativos com AWS QuickSight. Estou entusiasmado para aplicar este conhecimento em desafios futuros e continuar explorando novas capacidades na área de visualização e análise de dados.

[Sprint 10](Sprint%20Final%2010/README.md)
